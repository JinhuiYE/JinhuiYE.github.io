---
permalink: /

[//]: # (title: "Bio")
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% include base_path %}

Here is Jinhui!. I received my degree in [Software Engineering from South China University of Technology](http://www2.scut.edu.cn/sse/){: .no-underline } in 2022. Currently, I'm pursuing a MPhil degree at The [Hong Kong University of Science and Technology (Guangzhou)](https://hkust-gz.edu.cn/academics/four-hubs/information-hub/artificial-intelligence){: .no-underline } supervised By [Prof. Hui Xiong](https://scholar.google.com/citations?user=cVDF1tkAAAAJ&hl=zh-CN&oi=ao){: .no-underline } and [Junwei Liang](https://junweiliang.me/index.html){: .no-underline }.

My research interests lie in the intersection of action perception, manipulation and human-robotics interaction based on language. 
Previously, I interned at [Tencent AI Lab](https://ai.tencent.com/ailab/nlp/en/index.html){: .no-underline }, working on sign language translation in collaboration with [Xing Wang](http://xingwang4nlp.com/){: .no-underline } and [Wenxiang Jiao](https://wxjiao.github.io/){: .no-underline }. Currently, I'm part of the Multimodal Human-Computer Interaction Group at HKUST-GZ, focusing on multimodal human-computer interaction and collaboration, in partnership with PhD candidate [Weiyu Guo](https://guoweiyu.github.io/){: .no-underline }.

Recently, I am doing internship in [Stanford Vision and Learning Lab](http://vision.stanford.edu/){: .no-underline } working with the postdoc [Manling Li](https://limanling.github.io/){: .no-underline } and Prof [Jiajun Wu](https://jiajunwu.com/){: .no-underline }.

[//]: # ()
Moving forward, I aim to make a significant impact by connecting perception and reasoning in VLMs to robotic control.  Intuitively, I am particularly focused on efficiently mapping VLMs into finite instruction sets, which can drive physical agents. My ultimate goal is to advance intelligent systems that not only understand the world but can also interact with it in meaningful ways. 

**~~I am currently exploring potential opportunities for internships~~ I will do an on-site internship with Prof. [Yonatan](https://talkingtorobots.com/yonatanbisk.html) at CMU for the next term and I am looking for PhD positions for Fall 2025.**




News
------
• **0928**:
*Excited to announce that our two projects clinched 1st and 2nd places in the Human-Machine Interaction track of the Pazhou Algorithm Contest! We've been awarded a total of 120,000 RMB and look forward to a big celebratory dinner. 
The relative works will be documented in upcoming papers.* [link](https://mp.weixin.qq.com/s/_FuuvX1wKAW9dPBHi3yj8w)

[//]: # (• 0926:)

[//]: # (Participated in World Cleanup Day, collectively removing 17,970 items weighing 0.52 tons of marine debris. A fun and meaningful experience!)

• Before 08/30/2023: [...]


[//]: # (My mission is to conduct impactful and beneficial research that aids in bridging the gap between humans and AI. I envision a future where AI is not just seamlessly integrated into our lives, but also interacts with us in a real-time and autonomous manner.)

Publications
------
**LongvideoHaystack: Re-thinking Temporal Search for Long-Form Video Understandin**<br>
  **Jinhui Ye**, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, Jiajun Wu, Manling Li <br>
  CVPR 2025 . [[Project Page]](https://longvideohaystack.github.io/) [[Dataset]](https://huggingface.co/datasets/LVHaystack/LongVideoHaystack) Paper & Code coming soon <br>
**SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction**<br>
  Lu Dai, Yijie Xu, **Jinhui Ye**, Hao Liu, Hui Xiong <br>
  ICLR 2025<span style="color:red;">(spotlight)</span> . [[Openreview]](https://openreview.net/forum?id=ixMBnOhFGd2)  <br>
**Improving Gloss-free Sign Language Translation by Reducing Representation Density**<br>
  **Jinhui Ye**, Xing Wang, Wenxiang Jiao, Junwei Liang, Hui Xiong <br>
  NeurIPS 2024. [[arXiv]](https://arxiv.org/abs/2405.14312)  [[code]](https://github.com/JinhuiYE/SignCL) <br>
**Cross-modality Data Augmentation for End-to-End Sign Language Translation** <br>
  **Jinhui Ye**, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Hui Xiong <br>
  EMNLP 2023. [[arXiv]](https://arxiv.org/abs/2305.11096) [[code]](https://github.com/Atrewin/SignXmDA) <br>
**Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation**  <br>
  **Jinhui Ye**, Wenxiang Jiao, Xing Wang, Zhaopeng Tu <br>
  EACL 2023. [[paper]](https://aclanthology.org/2023.eacl-main.34/) [[code]](https://github.com/Atrewin/PGen) <br>
**Aspect-Opinion Correlation Aware and Knowledge-Expansion Few Shot Cross-Domain Sentiment Classification** <br>
  Haopeng Ren, Yi Cai, Yushi Zeng, **Jinhui Ye**, Ho-fung Leung,  Qing Li <br>
  Transactions on Affective Computing 2022. [[paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882094&casa_token=H2dOk5uWLXgAAAAA:Ex7FLRmkurlYY1x2rThmKne_NadSVUiOH2QcCx5IekFMxYYhF0wgGaf9DOXqFQdtGZPJGT9VNCiCGYs) [[code]](https://github.com/Atrewin/CroDomainFSSA)


On Processing Papers
------
**GeoDeformer: Geometric Deformable Transformer for Action Recognition** <br>
  **Jinhui Ye**, Jiaming Zhou, Hui Xiong, Junwei Liang <br>
  Preprint 2023. [[arXiv]](https://arxiv.org/abs/2311.17975) <br>
**Spatial-Temporal Alignment Network for Action Recognition** <br>
  **Jinhui Ye**, Junwei Liang <br>
  Preprint 2023. [[arXiv]](https://arxiv.org/pdf/2308.09897.pdf) <br>

Internship
------
• CMU LTI | Visiting Student | Sep. 2024 ‑ Present <br>
• Stanford AI Lab |  Intern | Dec. 2023 ‑ Oct. 2024 <br>
• Tencent AI Lab | Summer Intern | Aug. 2021 ‑ Aug. 2022

Awards
------

• Outstanding Undergraduate Thesis in SCUT <br>
• National Scholarship in China

