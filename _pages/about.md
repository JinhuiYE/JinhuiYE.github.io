---
permalink: /

[//]: # (title: "Bio")
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% include base_path %}

Here is Jinhui (叶劲辉), an incoming PhD student at [HKUST](https://seng.hkust.edu.hk/){: .no-underline} and supervised by Prof. [Jiaya Jia](https://jiaya.me/home). 
My research interests lie at the intersection of action perception, manipulation, and language-based human-robot interaction.

Previously, I obtained my degree from [South China University of Technology](http://www2.scut.edu.cn/sse/){: .no-underline} and pursued an MPhil at [The Hong Kong University of Science and Technology (Guangzhou)](https://hkust-gz.edu.cn/academics/four-hubs/information-hub/artificial-intelligence){: .no-underline} under the guidance of [Prof. Hui Xiong](https://scholar.google.com/citations?user=cVDF1tkAAAAJ&hl=zh-CN&oi=ao){: .no-underline} and [Junwei Liang](https://junweiliang.me/index.html){: .no-underline}. <br>
I have also had the opportunity to intern at several prestigious institutions:  
- At [Tencent AI Lab](https://ai.tencent.com/ailab/nlp/en/index.html){: .no-underline}, I worked on sign language translation in collaboration with [Xing Wang](http://xingwang4nlp.com/){: .no-underline} and [Wenxiang Jiao](https://wxjiao.github.io/){: .no-underline}.  <br>
- At [Stanford Vision and Learning Lab](http://vision.stanford.edu/){: .no-underline}, I collaborated with Prof [Manling Li](https://limanling.github.io/){: .no-underline}, Prof. [Jiajun Wu](https://jiajunwu.com/){: .no-underline} and Prof. [Li Fei-Fei](https://profiles.stanford.edu/fei-fei-li){: .no-underline}.  <br>
- At [CMU LIT](https://www.lti.cs.cmu.edu/){: .no-underline}, I worked with Prof. [Yonatan Bisk](https://talkingtorobots.com/yonatanbisk.html){: .no-underline} on Embodied-RAG. 

Recently, I have been interning at [Shanghai AI Lab](https://www.shlab.org.cn/){: .no-underline} with Dr. [Yilun Chen](https://yilunchen.com/about/){: .no-underline}, where our efforts are focused on aligning cognition and action within a Vision-Language-Action (VLA) framework. <br>
For my PhD, I plan to further explore this topic by developing a unified VLA model—a concept encapsulated in the Chinese phrase "知行合一" (the unity of knowledge and action, where a robot "does what it knows and knows what it is doing"). I am deeply passionate about this area and believe it holds great promise for both advancing research and enriching my personal philosophy. I welcome discussions and collaboration on these topics, so please feel free to reach out via email if you're interested.


News
------
• **0928**:
*Excited to announce that our two projects clinched 1st and 2nd places in the Human-Machine Interaction track of the Pazhou Algorithm Contest! We've been awarded a total of 120,000 RMB and look forward to a big celebratory dinner. 
The relative works will be documented in upcoming papers.* [link](https://mp.weixin.qq.com/s/_FuuvX1wKAW9dPBHi3yj8w)

[//]: # (• 0926:)

[//]: # (Participated in World Cleanup Day, collectively removing 17,970 items weighing 0.52 tons of marine debris. A fun and meaningful experience!)

• Before 08/30/2023: [...]


[//]: # (My mission is to conduct impactful and beneficial research that aids in bridging the gap between humans and AI. I envision a future where AI is not just seamlessly integrated into our lives, but also interacts with us in a real-time and autonomous manner.)

Publications
------

**FACE: A general Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens**<br>
  Chao Wang, Yixin Song, **Jinhui Ye**, Chuan Qin, Dazhong Shen, Lingfeng Liu, Xiang Wang, Yanyong Zhang  <br>
  NeurIPS 2025. [[paper]](https://openreview.net/pdf?id=loznSxLomv)<br>
**Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding**<br>
  Weiyu Guo, Ziyang Chen, Shaoguang Wang, Jianxiang He, Yijie Xu, **Jinhui Ye**, Ying Sun, Hui Xiong  <br>
  NeurIPS 2025. [[paper]](https://openreview.net/forum?id=yONFNHGoeP)<br>
**MolErr2Fix: Benchmarking LLM Trustworthiness in Chemistry via Modular Error Detection, Localization, Explanation, and Revision**<br>
  Yuyang Wu, **Jinhui Ye**, Shuhao Zhang, Lu Dai, Yonatan Bisk, Olexandr Isayev <br>
  EMNLP 2025<span style="color:red;">(Oral)</span> . [[paper]](https://arxiv.org/pdf/2509.00063)<br>
**LongvideoHaystack: Re-thinking Temporal Search for Long-Form Video Understandin**<br>
  **Jinhui Ye**, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, Jiajun Wu, Manling Li <br>
  CVPR 2025 . [[Project Page]](https://longvideohaystack.github.io/) [[Dataset]](https://huggingface.co/datasets/LVHaystack/LongVideoHaystack) <br>
**SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction**<br>
  Lu Dai, Yijie Xu, **Jinhui Ye**, Hao Liu, Hui Xiong <br>
  ICLR 2025<span style="color:red;">(spotlight)</span> . [[Openreview]](https://openreview.net/forum?id=ixMBnOhFGd2)  <br>
**Improving Gloss-free Sign Language Translation by Reducing Representation Density**<br>
  **Jinhui Ye**, Xing Wang, Wenxiang Jiao, Junwei Liang, Hui Xiong <br>
  NeurIPS 2024. [[arXiv]](https://arxiv.org/abs/2405.14312)  [[code]](https://github.com/JinhuiYE/SignCL) <br>
**Cross-modality Data Augmentation for End-to-End Sign Language Translation** <br>
  **Jinhui Ye**, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Hui Xiong <br>
  EMNLP 2023. [[arXiv]](https://arxiv.org/abs/2305.11096) [[code]](https://github.com/Atrewin/SignXmDA) <br>
**Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation**  <br>
  **Jinhui Ye**, Wenxiang Jiao, Xing Wang, Zhaopeng Tu <br>
  EACL 2023. [[paper]](https://aclanthology.org/2023.eacl-main.34/) [[code]](https://github.com/Atrewin/PGen) <br>
**Aspect-Opinion Correlation Aware and Knowledge-Expansion Few Shot Cross-Domain Sentiment Classification** <br>
  Haopeng Ren, Yi Cai, Yushi Zeng, **Jinhui Ye**, Ho-fung Leung,  Qing Li <br>
  Transactions on Affective Computing 2022. [[paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9882094&casa_token=H2dOk5uWLXgAAAAA:Ex7FLRmkurlYY1x2rThmKne_NadSVUiOH2QcCx5IekFMxYYhF0wgGaf9DOXqFQdtGZPJGT9VNCiCGYs) [[code]](https://github.com/Atrewin/CroDomainFSSA)


On Processing Papers
------
**GeoDeformer: Geometric Deformable Transformer for Action Recognition** <br>
  **Jinhui Ye**, Jiaming Zhou, Hui Xiong, Junwei Liang <br>
  Preprint 2023. [[arXiv]](https://arxiv.org/abs/2311.17975) <br>
**Spatial-Temporal Alignment Network for Action Recognition** <br>
  **Jinhui Ye**, Junwei Liang <br>
  Preprint 2023. [[arXiv]](https://arxiv.org/pdf/2308.09897.pdf) <br>

Internship
------
• CMU LTI | Visiting Student | Sep. 2024 ‑ Present <br>
• Stanford AI Lab |  Intern | Dec. 2023 ‑ Oct. 2024 <br>
• Tencent AI Lab | Summer Intern | Aug. 2021 ‑ Aug. 2022

Awards
------

• Outstanding Undergraduate Thesis in SCUT <br>
• National Scholarship in China

